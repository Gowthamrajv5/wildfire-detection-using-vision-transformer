# wildfire-detection-using-vision-transformer
https://peer.asee.org/wildfire-detection-using-vision-transformer-with-the-wildfire-dataset

We employed a ViT-Base-Patch16-224 model, implemented using the timm library in a Google Colab environment. The Wildfire Dataset, comprising 2,700 high-resolution aerial and ground images, was used for training. The dataset was split into training (80%), validation (10%), and test (10%) subsets. Each image was resized to 224x224 pixels, converted to tensors, and normalized using ImageNet statistics. PyTorch's ImageFolder and DataLoader were used to organize and batch the data.
Training was conducted using the Adam optimizer with a learning rate of 1e-4 and a batch size of 32. The CrossEntropyLoss function guided the optimization. The ViT model splits each image into 16x16 patches and processes them through layers of self-attention. After over 10 epochs, training loss decreased from 0.4510 to 0.0481, with validation accuracy stabilizing around 95.24%. Final test accuracy reached 96.10%. The model also achieved high precision (0.96 fire / 0.97 no-fire), recall (0.96 fire / 0.98 no-fire), and F1-score (~0.97). The ROC-AUC score was 0.97, indicating excellent classification capability. Inference time per batch was approximately 0.0068 seconds.
The model demonstrated consistent learning behavior, with training accuracy exceeding 95% and low overfitting. The confusion matrix confirmed 149 true positives, 241 true negatives, and only 10 false positives and false negatives each. Time complexity evaluations showed the model is suitable for real-time applications: each epoch took around 406.46 seconds on Google Colab with GPU support, with memory consumption under 1.1 GB.
Comparison with existing models showed superior performance of our ViT-based method. As per Table 1, our approach outperformed others like hybrid CNN-Transformer (91.8%), ViT + RNN (92.7%), and standard ViTs (92.5% - 93.2%). Table 2 showed our model surpassed ResNet-50 (88.5%), VGG-16 (85.3%), YOLOv5 (87.2%), and AlexNet (83.9%) with a top accuracy of 96.10%.
Misclassifications were primarily due to environmental interference such as smoke, cloud cover, and low-light conditions. These will be addressed in future iterations using augmented data and specialized pre-processing techniques.
